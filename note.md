Go to the path of the project and activate the Python virtual environment.

```shell
# python -m venv /home/raymondyan/hobby/PythonWebCrawler/LaobanPokemon
cd /home/raymondyan/hobby/PythonWebCrawler/LaobanPokemon/
source ./bin/activate
```

First page: 8153
Totle pages: 3419

```shell
scrapy shell 'https://www.iyingdi.com/tz/tool/general/pcards/8153'
scrapy shell 'https://www.iyingdi.com/tz/tool/general/pcards/8161'

scrapy shell 'https://www.iyingdi.com/tz/tool/general/pcards/8800'
```

```shell
cd tutorial
scrapy crawl pokemon -o output.json
scrapy crawl pokemon_reverse -o output_reverse.json
```

___

æ£€æŸ¥è®¿é—®çš„é¡µé¢æ˜¯å¦åŒ…å«æŒ‡å®šæ–‡æœ¬:

```python
print(response.text)
print("ä¸å¥½æ„æ€ï¼Œé¡µé¢è¢«æ³•å¸ˆæ’•æŽ‰äº†ï¼" not in response.text)
```

Extract the target description text:

```python
target_description_divs = response.xpath('//*[@id="__layout"]/div/div[3]/div/div/div[2]/div/div')
target_description_list = target_description_divs.css('div::text').getall()
"""
æˆ‘æœ‰ä»¥ä¸‹çš„ä¸€ä¸ªåˆ—è¡¨:
['èƒ½é‡è½¬ç§»', ' ', '\n          ç³»åˆ—ï¼šå¯¹æˆ˜æ´¾å¯¹ç»„åˆ è‰\n        ']

å¦‚ä½ æ‰€è§ï¼Œè¿™ä¸ªlistä¸­çš„å…ƒç´ æ˜¯å­—ç¬¦ä¸²ï¼Œå…¶ä¸­æŸäº›å­—ç¬¦ä¸²åªåŒ…å«ç©ºæ ¼å­—ç¬¦ï¼Œæˆ‘æƒ³åŽ»é™¤è¿™äº›åªåŒ…å«ç©ºæ ¼çš„å­—ç¬¦ä¸²å…ƒç´ ã€‚
"""
clean_target_description_list = [s for s in target_description_list if s.strip()]
print(clean_target_description_list)
# ðŸ‘‡ Output:
['èƒ½é‡è½¬ç§»', '\n          ç³»åˆ—ï¼šå¯¹æˆ˜æ´¾å¯¹ç»„åˆ è‰\n        ', '\n              ç±»åž‹ï¼š\n              ', 'è®­ç»ƒå®¶', '\n              å±žæ€§ï¼š\n              ', 'ä¸€èˆ¬', '\n              ç¨€æœ‰åº¦ï¼š\n              ', '-', '\n              æ•ˆæžœï¼š\n              ', 'å°†é™„ç€äºŽè‡ªå·±åœºä¸Šå®å¯æ¢¦èº«ä¸Šçš„1ä¸ªåŸºæœ¬èƒ½é‡ï¼Œè½¬é™„äºŽè‡ªå·±å…¶ä»–å®å¯æ¢¦èº«ä¸Šã€‚', '\n              ç”»å®¶ï¼š\n              ', 'Ryo Ueda', '\n              ç±»åˆ«ï¼š\n              ', 'ç‰©å“']


import re
clean_target_description_list = [re.sub(r"[ \t]+", "", s).replace("ï¼š", ":") for s in clean_target_description_list]
print(clean_target_description_list)
# ðŸ‘‡ Output:
['èƒ½é‡è½¬ç§»', '\nç³»åˆ—:å¯¹æˆ˜æ´¾å¯¹ç»„åˆè‰\n', '\nç±»åž‹:\n', 'è®­ç»ƒå®¶', '\nå±žæ€§:\n', 'ä¸€èˆ¬', '\nç¨€æœ‰åº¦:\n', '-', '\næ•ˆæžœ:\n', 'å°†é™„ç€äºŽè‡ªå·±åœºä¸Šå®å¯æ¢¦èº«ä¸Šçš„1ä¸ªåŸºæœ¬èƒ½é‡ï¼Œè½¬é™„äºŽè‡ªå·±å…¶ä»–å®å¯æ¢¦èº«ä¸Šã€‚', '\nç”»å®¶:\n', 'RyoUeda', '\nç±»åˆ«:\n', 'ç‰©å“']

# use a list comprehension to replace the ":\n" with ": " and merge the strings
concat_target_description_list = [s[:-1] + " " + clean_target_description_list[i+1] if s.endswith(":\n") else s for i, s in enumerate(clean_target_description_list)]
# print the new list
print(concat_target_description_list)
# ðŸ‘‡ Output:
['èƒ½é‡è½¬ç§»', '\nç³»åˆ—:å¯¹æˆ˜æ´¾å¯¹ç»„åˆè‰\n', '\nç±»åž‹: è®­ç»ƒå®¶', 'è®­ç»ƒå®¶', '\nå±žæ€§: ä¸€èˆ¬', 'ä¸€èˆ¬', '\nç¨€æœ‰åº¦: -', '-', '\næ•ˆæžœ: å°†é™„ç€äºŽè‡ªå·±åœºä¸Šå®å¯æ¢¦èº«ä¸Šçš„1ä¸ªåŸºæœ¬èƒ½é‡ï¼Œè½¬é™„äºŽè‡ªå·±å…¶ä»–å®å¯æ¢¦èº«ä¸Šã€‚', 'å°†é™„ç€äºŽè‡ªå·±åœºä¸Šå®å¯æ¢¦èº«ä¸Šçš„1ä¸ªåŸºæœ¬èƒ½é‡ï¼Œè½¬é™„äºŽè‡ªå·±å…¶ä»–å®å¯æ¢¦èº«ä¸Šã€‚', '\nç”»å®¶: RyoUeda', 'RyoUeda', '\nç±»åˆ«: ç‰©å“', 'ç‰©å“']
# â˜ Note that some elements in the list above are subsets of the previous element. For example: 'è®­ç»ƒå®¶' is a subset of '\nç±»åž‹: è®­ç»ƒå®¶'.

# use a list comprehension to remove the elements that are subsets of other elements
clean_concat_target_description_list = [s for s in concat_target_description_list if not any(s in t for t in concat_target_description_list if s != t)]
# print the new list
print(clean_concat_target_description_list)

# Finally, we can combine the elements of the list to a single string
combine_target_description_str = "".join(clean_concat_target_description_list)
print(combine_target_description_str)
```

Extract the target image:

```python
target_img_selector = response.css("#__layout > div > div.default-main-container.w-full.pl-90.big\:pl-250.pt-56 > div > div > div.marvel-card-detail-page > div > img")

# Extract the relative URL of the image from the Selector
relative_url = target_img_selector.css("::attr(src)").get()
print(relative_url)

# Construct the absolute URL of the image
from urllib.parse import urljoin
absolute_url = urljoin(response.url, relative_url)
print(absolute_url)

target_img_alt = target_img_selector.css('::attr(alt)').get()
print(target_img_alt)

# Send a GET request to download the image
import requests
response = requests.get(absolute_url)

# Check if the request was successful
if response.status_code == 200:
    # Save the image to a file
    with open(f'./crawled_images/{target_img_alt}.png', 'wb') as file:
        file.write(response.content)
        print(f"Image downloaded successfully. URL: {absolute_url}. ImageFileName: {target_img_alt}.png.")
else:
    print(f"Failed to download the image.  URL: {absolute_url}. ImageFileName: {target_img_alt}.png.")
```
